---
title: 'Homework 3'
header-includes:
  - \usepackage{multirow}
output:
  pdf_document: default
urlcolor: blue
---

```{r, include=FALSE}

library(tidyverse)
knitr::opts_chunk$set(tidy = FALSE)
```

## Context

This assignment reinforces ideas in Module 3: Cluster computing.


## Due date and submission

Please submit (via Canvas) a PDF containing a link to the web address of the GitHub repo containing your work for this assignment; git commits after the due date will cause the assignment to be considered late. 



## Points

```{r, echo = FALSE}
tibble(
  Problem = c("Problem 0", "Problem 1"),
  Points = c(20, 80)
) %>%
  knitr::kable()
```


## Problem 0 

This “problem” focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

To that end:

* Create a public GitHub repo + local R Project
* Submit your whole project folder to GitHub 
* Submit a PDF knitted from Rmd to Canvas. Your solutions to the problems here should be implemented in your .Rmd file, and your git commit history should reflect the process you used to solve these Problems.

The Github address: https://github.com/ytliu36/bios731_hw3_liu.git

## Problem 1

Continuation of Homework 1. Here, we will re-run part of the simulation study from Homework 1 with some minor changes, on the cluster. Cluster computing space is limited so we will not run too many jobs or simulations.  

### Problem 1 setup

The simulation study is specified below:

Below is a multiple linear regression model, where we are interested in primarily treatment effect.


$$Y_i = \beta_0 + \beta_{treatment}X_{i1} + \mathbf{Z_i}^T\boldsymbol{\gamma} + \epsilon_i$$


Notation is defined below: 

* $Y_i$: continuous outcome
* $X_{i1}$: treatment group indicator; $X_{i1}=1$ for treated 
* $\mathbf{Z_i}$: vector of potential confounders
* $\beta_{treatment}$: average treatment effect, adjusting for $\mathbf{Z_i}$
* $\boldsymbol{\gamma}$: vector of regression coefficient values for confounders 
* $\epsilon_i$: errors, we will vary how these are defined


In our simulation, we want to 

* Estimate $\beta_{treatment}$
  * Evaluate $\beta_{treatment}$ through bias, coverage, type 1 error, and power
  * We will use 2 methods to compute $se(\hat{\beta}_{treatment})$ and coverage:
    1. Wald confidence intervals (the standard approach)
    2. Nonparametric bootstrap percentile intervals
  * Evaluate computation times for each method to compute a confidence interval

* Evaluate these properties at:
  - Sample size $n =\{20\}$
  - True values $\beta_{treatment} \in \{0, 0.5\}$
  - True $\epsilon_i$ normally distributed with $\epsilon_i \sim N(0, 2)$
  - True $\epsilon_i$ coming from a highly right skewed distribution
    - Generate data from a Gamma distribution with `shape = 1` and `rate = 2`.

* Assume that there are no confounders ($\boldsymbol{\gamma} = 0$)
* Use a full factorial design
* Use same `nsim` as previous assignment.


### Problem 1 tasks

We will execute this full simulation study. For full credit, make sure to implement the following:

**Workflow:**
* Use structured scripts and subfolders following guidance from the cluster computing project organization lecture
* Instead of parallelizing your simulation scenarios (as in HW1), each simulation scenario should be assigned a different JOBID on the cluster.


**Presenting results:**

Create plots with *Monte Carlo standard error bars* to summarize the following:

- Bias of $\hat{\beta}$

```{r echo=FALSE}
library(ggplot2)

# Function to extract biases from an RDA file
extract_bias <- function(file) {
  load(here::here("data", file))
  wald_bias <- sapply(results, function(x) x[1]) 
  boot_bias <- sapply(results, function(x) x[2]) 
  data.frame(
    Method = rep(c("Wald", "Bootstrap"), each = length(wald_bias)),
    Bias = c(wald_bias, boot_bias)
  )
}

# Load all scenarios
scenarios <- lapply(1:4, function(i) extract_bias(paste0("scenario_", i, ".RDA")))

# Combine into a single dataframe
bias_data <- do.call(rbind, Map(cbind, scenarios, Scenario = paste0("Scenario ", 1:4)))

# Compute means and MCSE
summary_data <- aggregate(Bias ~ Method + Scenario, data = bias_data, FUN = function(x) {
  c(mean = mean(x), mcse = sd(x) / sqrt(length(x)))
})

# Convert list columns to separate columns
summary_data <- do.call(data.frame, summary_data)
colnames(summary_data) <- c("Method", "Scenario", "Mean", "MCSE")

# Create the plot
ggplot(summary_data, aes(x = Scenario, y = Mean, fill = Method)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(ymin = Mean - MCSE, ymax = Mean + MCSE), 
                width = 0.2, position = position_dodge(0.9)) +
  labs(title = "Bias Comparison with Monte Carlo Standard Error",
       y = "Bias", x = "Scenario") +
  theme_minimal()
```

- Coverage of $\hat{\beta}$

```{r echo=FALSE}
# Function to extract coverage from the RDA files
extract_coverage <- function(file) {
  load(here::here("data", file))
  wald_coverage <- sapply(results, function(x) x[5]) 
  boot_coverage <- sapply(results, function(x) x[6])  
  
  data.frame(
    Method = rep(c("Wald", "Bootstrap"), each = length(wald_coverage)),
    Coverage = c(wald_coverage, boot_coverage)
  )
}

# Load all scenarios for coverage
coverage_scenarios <- lapply(1:4, function(i) extract_coverage(paste0("scenario_", i, ".RDA")))

# Combine coverage data into a single dataframe
coverage_data <- do.call(rbind, Map(cbind, coverage_scenarios, Scenario = paste0("Scenario ", 1:4)))

# Compute means and MCSE for coverage
coverage_summary_data <- aggregate(Coverage ~ Method + Scenario, data = coverage_data, FUN = function(x) {
  c(mean = mean(x), mcse = sd(x) / sqrt(length(x)))
})

# Convert list columns to separate columns
coverage_summary_data <- do.call(data.frame, coverage_summary_data)
colnames(coverage_summary_data) <- c("Method", "Scenario", "Mean", "MCSE")

# Create the plot for coverage
ggplot(coverage_summary_data, aes(x = Scenario, y = Mean, fill = Method)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(ymin = Mean - MCSE, ymax = Mean + MCSE), 
                width = 0.2, position = position_dodge(0.9)) +
  labs(title = "Coverage Comparison with Monte Carlo Standard Error",
       y = "Coverage", x = "Scenario") +
  theme_minimal()
```

- Power

Under alternative assumption $\beta_{treatment} \neq 0$, simulation scenarios 2 and 4, we have:

```{r echo=FALSE}
# Function to extract coverage from the RDA files and compute Type I error
extract_power_from_coverage <- function(file) {
  load(here::here("data", file))
  wald_coverage <- sapply(results, function(x) x[7])  
  boot_coverage <- sapply(results, function(x) x[8]) 
  
  # Compute Type I error as 1 - coverage
  wald_type1_error <-wald_coverage
  boot_type1_error <-boot_coverage
  
  data.frame(
    Method = rep(c("Wald", "Bootstrap"), each = length(wald_type1_error)),
    Power = c(wald_type1_error, boot_type1_error)
  )
}

# Load only the scenarios under null hypothesis (Scenario 1 and Scenario 3)
power_scenarios <- lapply(c(2, 4), function(i) extract_power_from_coverage(paste0("scenario_", i, ".RDA")))

# Combine Type I error data into a single dataframe
power_data <- do.call(rbind, Map(cbind, power_scenarios, Scenario = paste0("Scenario ", c(2, 4))))

# Compute means and MCSE for Type I error
power_summary_data <- aggregate(Power ~ Method + Scenario, data = power_data, FUN = function(x) {
  c(mean = mean(x), mcse = sd(x) / sqrt(length(x)))
})

# Convert list columns to separate columns
power_summary_data <- do.call(data.frame, power_summary_data)
colnames(power_summary_data) <- c("Method", "Scenario", "Mean", "MCSE")

# Create the plot for Type I error
ggplot(power_summary_data, aes(x = Scenario, y = Mean, fill = Method)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(ymin = Mean - MCSE, ymax = Mean + MCSE), 
                width = 0.2, position = position_dodge(0.9)) +
  labs(title = "Power Comparison (Based on Coverage of 0) with Monte Carlo Standard Error",
       y = "Power Rate", x = "Scenario") +
  theme_minimal()

```

- Type 1 error

Under null assumption $\beta_{treatment} = 0$, simulation scenarios 1 and 3, we have:

```{r echo=FALSE}
# Function to extract coverage from the RDA files and compute Type I error
extract_type1_error_from_coverage <- function(file) {
  load(here::here("data", file))
  wald_coverage <- sapply(results, function(x) x[7])  
  boot_coverage <- sapply(results, function(x) x[8]) 
  
  # Compute Type I error as 1 - coverage
  wald_type1_error <- 1 - wald_coverage
  boot_type1_error <- 1 - boot_coverage
  
  data.frame(
    Method = rep(c("Wald", "Bootstrap"), each = length(wald_type1_error)),
    Type1Error = c(wald_type1_error, boot_type1_error)
  )
}

# Load only the scenarios under null hypothesis (Scenario 1 and Scenario 3)
type1_error_scenarios <- lapply(c(1, 3), function(i) extract_type1_error_from_coverage(paste0("scenario_", i, ".RDA")))

# Combine Type I error data into a single dataframe
type1_error_data <- do.call(rbind, Map(cbind, type1_error_scenarios, Scenario = paste0("Scenario ", c(1, 3))))

# Compute means and MCSE for Type I error
type1_error_summary_data <- aggregate(Type1Error ~ Method + Scenario, data = type1_error_data, FUN = function(x) {
  c(mean = mean(x), mcse = sd(x) / sqrt(length(x)))
})

# Convert list columns to separate columns
type1_error_summary_data <- do.call(data.frame, type1_error_summary_data)
colnames(type1_error_summary_data) <- c("Method", "Scenario", "Mean", "MCSE")

# Create the plot for Type I error
ggplot(type1_error_summary_data, aes(x = Scenario, y = Mean, fill = Method)) +
  geom_col(position = "dodge") +
  geom_errorbar(aes(ymin = Mean - MCSE, ymax = Mean + MCSE), 
                width = 0.2, position = position_dodge(0.9)) +
  labs(title = "Type I Error Comparison (Based on Coverage of 0) with Monte Carlo Standard Error",
       y = "Type I Error Rate", x = "Scenario") +
  theme_minimal()

```


Write 1-2 paragraphs summarizing these results.

Based on the results above we have following results:

1. Estimates from normal error are almost non-bias, while we tend to overestimate under gamma error, because gamma random errors are always positive. Meanwhile, the standard error in normal is larger than gamma, which is because gamma with shape 1 rate 2 has a smaller variance than normal N(0,2).

2. Wald tend to have a better coverage than Bootstrap under the 4 scenarios

3. In terms of hypothesis testing, the power of both methods under normal error have a much better power compared to gamma error scenario.

4. Only under gamma error the wald estimate provide a type I error around 0.05, while all others are over 0.05, bootstrap is getting a larger type I error compared to wald uder both scenarios.


